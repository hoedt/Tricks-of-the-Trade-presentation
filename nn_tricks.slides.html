<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Neural Networks: Tricks of the Trade</title>

		<link rel="stylesheet" href="https://unpkg.com/reveal.js/dist/reset.css">
		<link rel="stylesheet" href="https://unpkg.com/reveal.js/dist/reveal.css">
		<link rel="stylesheet" href="https://unpkg.com/reveal.js/dist/theme/white.css">
		<style>
            .reveal footer {
                position: absolute; 
                bottom: 0; left: 0; right: 0; 
                width: var(--slide-width); 
                margin: auto;
                perspective: 600px;
                perspective-origin: 50% 40%;
            }
            [class~="no-footer"] footer { display: none; }
            .reveal .slides section { text-align: left; }
            .reveal .slides .landing { text-align: center; }
            .reveal .slides h1 { font-size: 73pt; line-height: 1; }
            .reveal .slides h2 { font-size: 53pt; line-height: 1; }
            .reveal .slides h3 { font-size: 53pt; line-height: 1; font-weight: normal; }
            .reveal .slides ul { list-style-type: square; }
            .reveal .slides li.irrelevant { color: #aaa; }
			.reveal .slides figure { background-color: white; }
            .reveal .slides figcaption, .citation { font-size: 10pt; text-align: center; }
			.reveal .slides .citation { position: absolute; bottom: 0px; right: 0px; }
			.reveal .slides .references > p { font-size: 8pt; margin: .5% 0;}
        </style>

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="https://unpkg.com/reveal.js/plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<main class="slides">
				<section class="landing" data-state="no-footer">
					<a href="https://www.jku.at">
						<img src="https://www.jku.at/fileadmin/marketing/Startseite/JKU-Hauptlogo-en-schwarz-hoch.svg" 
							alt="JKU logo" style="height: 160px;" class="jku-logo">
					</a>
				</section>

				<section class="title">
					<h1>Neural networks: tricks of the trade</h1>
					<svg xmlns="http://wwww.w3.org/2000/svg" width="184" viewBox="0 0 185.5 168.8">
						<path d="M 185.4,0 92.7,79.5 0,0 V 49 L 92.7,128.4 185.4,49 Z m 0,128.5 H 92.7 0 v 40.3 h 185.5 v -40.3 z" />
					</svg>
					<p>
						<small>
							Pieter-Jan Hoedt @ AIDD 2021 <br />
							<a href="mailto:hoedt@ml.jku.at">
								<code>hoedt@ml.jku.at</code>
							</a>
						</small>
					</p>
				</section>

				<section>
					<h2>Motivation</h2>
					<div class="r-stretch r-hstack justify-around">
						<figure>
							<blockquote cite="Orr, G. B., & Müller, K.-R. (Eds.). (1998). 
								Neural Networks: Tricks of the Trade (1st ed., Vol. 1524). Springer." style="width: 400">
								newcomers to the field waste much time wondering 
								why their networks train so slowly and perform so poorly
							</blockquote>
							<figcaption>
								(<a href="#orr98tricks">Orr & Müller, 1998</a>)
							</figcaption>
						</figure>
						<figure>
							<img src="https://media.springernature.com/w306/springer-static/cover-hires/book/978-3-540-49430-0"
									alt="cover page of the book 'Neural Networks: Tricks of the Trade'" width="600" />
							<figcaption>
								cover of the 1998 edition
							</figcaption>
						</figure>
					</div>
					<aside class="notes">
						- chapter 1 most impactful<br/>
						- 2nd edition, more chapters<br/>
						- incomplete set of tricks
					</aside>
				</section>

				<section>
					<h2>Inspiration</h2>
					<div class="r-stack r-stretch">
						<iframe src="http://karpathy.github.io/2019/04/25/recipe/" width="70%" height="80%"
						        title="A Recipe for Training Neural Networks">
							A recipe for Training Neural Networks (by Andrej Karpathy)
						</iframe>
					</div>
					<aside class="notes">
						- more hands-on approach
					</aside>
				</section>

				<section data-auto-animate>
					<h2>Outline</h2>
					<div class="r-stretch r-stack">
						<ol>
							<li>Data</li>
							<li>Model</li>
							<li>Learning</li>
						</ol>
					</div>
				</section>

				<section>
					<section data-auto-animate>
						<h2>Outline</h2>
						<div class="r-stretch r-stack">
							<ol>
								<li>Data</li>
								<li class="irrelevant">Model</li>
								<li class="irrelevant">Learning</li>
							</ol>
						</div>
					</section>

					<section>
						<h2>Raw <br /> Data</h2>
						<div class="r-stretch r-vstack">
							<figure>
								<img src="https://www.nist.gov/sites/default/files/styles/2800_x_2800_limit/public/images/2019/04/27/sd19.jpg?itok=f_E9AQtX"
									alt="NIST form to collect handwritten character data" width="350" />
								<figcaption>
									Data form for NIST Special Database 19 
									(<a href="#grother95nist">Grother, 1995</a>)
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- training data collected from paid workers<br/>
							- test data collected from college students<br/>
							- canonical representation<br/>
							=> bad sampling
						</aside>
					</section>

					<section>
						<h2>Preprocessed <br /> Data</h2>
						<div class="r-stack r-stretch">
							<figure>
								<img src="resources/mnist_samples_raw.png"
									alt="examples of MNIST digits" />
								<figcaption>
									MNIST digits as 8-bit grayscale images
									(<a href="#bottou94mnist">Bottou et al., 1994</a>;
									<a href="#yadav19qmnist">Yadav & Bottou, 2019</a>)
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- resampled dataset<br/>
							- importance of sampling (e.g. shuffled data)<br/>
							- sampling to counter class imbalance
							- sampling through emphasising scheme (chapter 1)<br/>
						</aside>
					</section>

					<section>
						<h2>Looking at <br /> Data</h2>
						<div class="r-stack r-stretch">
							<figure class="r-vstack justify-around r-stretch" style="opacity: .5">
								<img src="resources/mnist_samples_raw.png"
									alt="examples of MNIST digits" />
								<figcaption>
									MNIST digits as 8-bit grayscale images
									(<a href="#bottou94mnist">Bottou et al., 1994</a>;
									 <a href="#yadav19qmnist">Yadav & Bottou, 2019</a>)
								</figcaption>
							</figure>
							<pre style="width: 80%"><code class="python" data-trim data-line-numbers="1-3|5-7">
								mean = np.mean(mnist, axis=0)
								plt.imshow(mean)
								plt.show()

								std = np.std(mnist, axis=0)
								plt.imshow(std)
								plt.show()
							</code></pre>
						</div>
						<aside class="notes">
							- naive visualisation of Statistics<br/>
							- same colors implies similar quantities
						</aside>
					</section>

					<section>
						<h2>Looking at <br /> Data</h2>
						<div class="r-stack r-stretch">
							<figure class="r-vstack justify-around r-stretch" style="opacity: .5">
								<img src="resources/mnist_samples_raw.png"
									alt="examples of MNIST digits" />
								<figcaption>
									MNIST digits as 8-bit grayscale images
									(<a href="#bottou94mnist">Bottou et al., 1994</a>;
									 <a href="#yadav19qmnist">Yadav & Bottou, 2019</a>)
								</figcaption>
							</figure>
							<pre style="width: 80%"><code class="python" data-trim data-line-numbers="2|6-7">
								mean = np.mean(mnist, axis=0)
								plt.imshow(mean, vmin=0, vmax=255, cmap='gray')
								plt.show()

								std = np.std(mnist, axis=0)
								plt.imshow(std, vmin=0, cmap='viridis')
								plt.colorbar()
								plt.show()
							</code></pre>
						</div>
						<aside class="notes">
							- better visualisation of data<br/>
							- bounds to display values correctly<br/>
							- colorbar if bounds not known
						</aside>
					</section>

					<section>
						<h2>Looking at <br /> Data</h2>
						<div class="r-hstack justify-around r-stretch">
							<figure>
								<img src="resources/mnist_samples_raw.png"
										alt="examples of scaled MNIST digits" />
								<figcaption>
									MNIST digits as 8-bit grayscale images
								</figcaption>
							</figure>
							<figure>
								<img src="resources/mnist_stats_raw.png"
										alt="mean and standard deviation over scaled MNIST digits" />
								<figcaption>
									Statistics over 8-bit MNIST digits
								</figcaption>
							</figure>
						</div>
					</section>

					<section>
						<h2>Looking at <br /> Data</h2>
						<div class="r-hstack justify-around r-stretch">
							<figure>
								<!-- <img src="https://www.researchgate.net/publication/346766201/figure/fig2/AS:966671690903554@1607483899632/Application-of-t-SNE-UMAP-TriMAP-and-PaCMAP-to-the-MNIST-dataset-Algorithms-that.ppm"
										alt="comparison of different dimensionality reduction methods on MNIST" width="600" /> -->
								<img src="resources/dim_reduction.png"
										alt="comparison of different dimensionality reduction methods on MNIST" width="600" />
								<figcaption>
									Different dimensionality reduction methods on MNIST digits 
									(<a href="#wang21pacmap">Wang et al., 2021</a>)
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- dimensionality reduction is an unsupervised technique<br/>
							- especially useful for non-image data.
						</aside>
					</section>

					<section>
						<h2>Statistics of the <br /> Data</h2>
						<div class="r-hstack justify-around r-stretch">
							<figure>
								<img src="resources/mnist_samples_raw.png"
										alt="examples of scaled MNIST digits" />
								<figcaption>
									MNIST digits as 8-bit grayscale images
								</figcaption>
							</figure>
							<figure>
								<img src="resources/mnist_stats_raw.png"
										alt="mean and standard deviation over scaled MNIST digits" />
								<figcaption>
									Statistics over 8-bit MNIST digits
								</figcaption>
							</figure>
						</div>
					</section>

					<section>
						<h2>Transforming <br /> Data</h2>
						<div class="r-hstack justify-around r-stretch">
							<figure>
								<img src="resources/mnist_samples_float.png"
										alt="examples of scaled MNIST digits" />
								<figcaption>
									MNIST digits as floating point values $\in [0, 1]$
								</figcaption>
							</figure>
							<figure>
								<img src="resources/mnist_stats_float.png"
										alt="mean and standard deviation over scaled MNIST digits" />
								<figcaption>
									Statistics over $[0, 1]$ MNIST digits
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- uint8 is not DL-ready<br/>
							- simply map values to [0, 1]
						</aside>
					</section>

					<section>
						<h2>Centred <br /> Data</h2>
						<div class="r-hstack justify-around r-stretch">
							<figure>
								<img src="resources/mnist_samples_pxl_centred.png"
										alt="examples of centred MNIST digits" />
								<figcaption>
									Mean-centred MNIST digits
								</figcaption>
							</figure>
							<figure>
								<img src="resources/mnist_stats_pxl_centred.png"
										alt="mean and standard deviation over centred MNIST digits" />
								<figcaption>
									Statistics over centred MNIST digits
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- zero mean data is better (see later)<br/>
							- note negative value blob
						</aside>
					</section>

					<section>
						<h2>Normalised <br /> Data</h2>
						<div class="r-hstack justify-around r-stretch">
							<figure>
								<img src="resources/mnist_samples_pxl_normalised.png" width="588px" height="314px"
										alt="examples of normalised MNIST digits" />
								<figcaption>
									Mean- and variance-normalised MNIST digits
								</figcaption>
							</figure>
							<figure>
								<img src="resources/mnist_stats_pxl_normalised.png" width="256px" height="418px"
									 alt="mean and standard deviation over normalised MNIST digits" />
								<figcaption>
									Statistics over normalised MNIST digits
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- unit variance data is better (see later)<br/>
							- note outliers
						</aside>
					</section>

					<section>
						<h2>Normalised <br /> Data</h2>
						<div class="r-hstack justify-around r-stretch">
							<figure>
								<img src="resources/mnist_samples_pxl_normalised_clipped.png"
										alt="examples of clipped normalised MNIST digits" />
								<figcaption>
									Mean- and variance-normalised MNIST digits (clipped)
								</figcaption>
							</figure>
							<figure>
								<img src="resources/mnist_stats_pxl_normalised_clipped.png"
										alt="clipped mean and standard deviation over normalised MNIST digits" />
								<figcaption>
									Statistics over normalised MNIST digits (clipped)
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- using 99% quantile for clipping<br/>
							- note bright spots indicating rare occurences
						</aside>
					</section>

					<section>
						<h2>PCA-whitened <br /> Data</h2>
						<div class="r-hstack justify-around r-stretch">
							<figure>
								<img src="resources/mnist_samples_pxl_pca_clipped.png"
										alt="examples of PCA-whitened MNIST digits" />
								<figcaption>
									PCA-whitened MNIST digits (clipped)
								</figcaption>
							</figure>
							<figure>
								<img src="resources/mnist_stats_pxl_pca_clipped.png"
										alt="mean and standard deviation over PCA-whitened MNIST digits" />
								<figcaption>
									Statistics over PCA-whitened MNIST digits (clipped)
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- PCA = eigenvalue decomposition of covariance matrix<br/>
							- identity covariance data is better (see later)<br/>
							- note shuffled pixels (sorted by eigenvalue)
						</aside>
					</section>

					<section>
						<h2>ZCA-whitened <br /> Data</h2>
						<div class="r-hstack justify-around r-stretch">
							<figure>
								<img src="resources/mnist_samples_pxl_zca_clipped.png"
										alt="examples of ZCA-whitened MNIST digits" />
								<figcaption>
									ZCA-whitened MNIST digits (clipped)
								</figcaption>
							</figure>
							<figure>
								<img src="resources/mnist_stats_pxl_zca_clipped.png"
										alt="mean and standard deviation over ZCA-whitened MNIST digits" />
								<figcaption>
									Statistics over ZCA-whitened MNIST digits (clipped)
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- rotate PCA back to original space
							- corresponds to multiplying with inverse sqrt of covariance<br/>
						</aside>
					</section>

					<section>
						<h2>Why normalise/whiten? <br /> Data</h2>
						<div class="r-vstack justify-around r-stretch">
							<p>
								$$\begin{aligned}
								  \mathrm{E}_{\vec{w}}
								  &= \frac{1}{|\mathcal{D}|} \sum_{\vec{x} \in \mathcal{D}} \frac{1}{2} (\vec{w} \cdot \vec{x} - y)^2 \\
								  \operatorname{\nabla} \mathrm{E}_{\vec{w}}
								  &= \frac{1}{|\mathcal{D}|} \sum_{\vec{x} \in \mathcal{D}} (\vec{w} \cdot \vec{x} - y) \vec{x} \\
								  \operatorname{\nabla}^2 \mathrm{E}_{\vec{w}}
								  &= \frac{1}{|\mathcal{D}|} \sum_{\vec{x} \in \mathcal{D}} \vec{x} \otimes \vec{x}
								\end{aligned}$$
							</p>
						</div>
						<p class="citation">
							Taken from section 5.1 in <a href="#lecun98efficient">(LeCun et al., 1998)</a>
						</p>
						<aside class="notes">
							- example given: linear regression <br/>
							- Hessian (second derivative) = covariance matrix<br/>
							- similar results hold for logistic regression<br/>
							- similar results hold for multi-layer fully-connected networks
						</aside>
					</section>

					<section>
						<h2>Why normalise/whiten? <br /> Data</h2>
						<div class="r-vstack justify-around r-stretch">
							<p>
								$\begin{aligned}
								  \mathrm{ E}_{\vec{w}^*}
								  &\approx \mathrm{E}_{\vec{w}} + \operatorname{\nabla} \mathrm{E}_{\vec{w}} \cdot (\vec{w}^* - \vec{w}) \\ %+ \frac{1}{2} \|\vec{w}^* - \vec{w}\|_{\operatorname{\nabla}^2 \mathrm{E}_{\vec{w}}}^2 \\
								  \operatorname{\nabla} \mathrm{E}_{\vec{w}^*} 
								  &\approx \operatorname{\nabla} \mathrm{E}_{\vec{w}} + \operatorname{\nabla}^2 \mathrm{E}_{\vec{w}} \cdot (\vec{w}^* - \vec{w})
								\end{aligned}$
							</p>
							<p class="fragment">
								$$\vec{w}^* \approx \vec{w} - \bigg(\operatorname{\nabla}^2 \mathrm{E}_{\vec{w}}\bigg)^{-1}\operatorname{\nabla} \mathrm{E}_{\vec{w}}$$
							</p>
						</div>
						<p class="citation">
							Taken from section 5.1 in (<a href="#lecun98efficient">LeCun et al., 1998</a>)
						</p>
						<aside class="notes">
							- second order Taylor approximation<br/>
							- Hessian defines optimal learning rate<br/>
							- low singular values = hard to optimise direction<br/>
							- can be used to control importance of features
						</aside>
					</section>

					<section>
						<h2>Why <em>not</em> to whiten? <br /> Data</h2>
						<div class="r-vstack justify-around r-stretch">
							<img src="resources/whitening_bad.png" class="fragment"
							    alt="Whitening and Second Order Optimization Both make Information in the 
								     Dataset Unusable During Training, and Can Reduce or Prevent Generalization" />
							<p class="fragment">
								<strong>BUT:&emsp;</strong>
								$\mat{W}_\mathrm{ZCA} = \mat{C}^{-1/2}$
							</p>
						</div>
						<p class="citation">
							<a href="#wadia21whitening">(Wadia et al., 2021)</a>
						</p>
						<aside class="notes">
							- Whitening transform expensive to compute!<br/>
							- Whitening transform computed from training samples<br/>
							- intuitive this could be bad for generalisation<br/>
							- BUT fixed (often invertible) linear transformation
						</aside>
					</section>

					<section>
						<h2>Normalisation of <br /> Data</h2>
						<div class="r-hstack justify-around r-stretch">
							<figure>
								<img src="resources/mnist_samples_pxl_normalised_clipped.png"
										alt="examples of clipped normalised MNIST digits" />
								<figcaption>
									Mean- and variance-normalised MNIST digits (clipped)
								</figcaption>
							</figure>
							<figure>
								<img src="resources/mnist_stats_pxl_normalised_clipped.png"
										alt="clipped mean and standard deviation over normalised MNIST digits" />
								<figcaption>
									Statistics over normalised MNIST digits (clipped)
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- inductive bias<br/>
							- per-pixel normalisation breaks local structure<br/>
							- good for fully connected, but bad for convolutions
						</aside>
					</section>

					<section>
						<h2>Practical Normalisation of <br /> Data</h2>
						<div class="r-hstack justify-around r-stretch">
							<figure>
								<img src="resources/mnist_samples_img_normalised.png"
										alt="examples of channel-normalised MNIST digits" />
								<figcaption>
									Mean- and variance-normalised MNIST digits (channel)
								</figcaption>
							</figure>
							<figure>
								<img src="resources/mnist_stats_img_normalised.png"
										alt="mean and standard deviation over channel-normalised MNIST digits" />
								<figcaption>
									Statistics over normalised MNIST digits (channel)
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- per-channel normalisation does not break local structure<br/>
							- pixels as samples (instead of features)<br/>
							- mainly due to convolutional networks
						</aside>
					</section>

					<section>
						<h2>Invariances in <br /> Data</h2>
						<div class="r-hstack justify-around r-stretch">
							<figure>
								<img src="resources/mnist_samples_augmented.png"
										alt="example augmentations of an MNIST digit" />
								<figcaption>
									Example augmentations of an MNIST digit
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- augmentations can be used to model invariances<br/>
							- can change distribution!<br/>
							- make sure that invariance makes sense
						</aside>
					</section>

					<section>
						<h2>TL;DR <br /> Data</h2>
						<div class="r-stretch r-stack">
							<ul>
								<li>be aware of <strong>origins</strong></li>
								<li>gain <strong>insights</strong> (e.g. using visualisation)</li>
								<li>use <strong>statistics</strong> to your advantage</li>
								<li>consider <strong>inductive biases</strong> of the model</li>
							</ul>
						</div>
						<aside class="notes">
							- origin: where does data come from and how to get a canonical represenation<br/>
							- insights: get your hands dirty and get a feeling for the data<br/>
							- statistics: identity covariance => universal lr<br/>
							- inductive bias: use normalisation and augmentation carefully
						</aside>
					</section>
				</section>

				<section>
					<section data-auto-animate>
						<h2>Outline</h2>
						<div class="r-stretch r-stack">
							<ol>
								<li class="irrelevant">Data</li>
								<li>Model</li>
								<li class="irrelevant">Learning</li>
							</ol>
						</div>
					</section>
					<section>
						<h2>Baseline<br/>Model</h2>
						<div class="r-stretch r-stack">
							<blockquote class="twitter-tweet" data-cards="hidden" data-theme="dark">
								<p lang="en" dir="ltr">
									My new hobby is seeing what results I can get on MNIST using k-nearest neighbors. 
									As a baseline kNN gets 96% accuracy without using any special tricks.<br>96%!<br>
									<br>The code:<a href="https://t.co/sMAgNuAPiP">https://t.co/sMAgNuAPiP</a><br>
									k = 5, L1 distance (sum of the absolute value of the difference of the pixel values)
								</p>
								&mdash; Brandon Rohrer (@_brohrer_) 
								<a href="https://twitter.com/_brohrer_/status/1397310565312303107?ref_src=twsrc%5Etfw">May 25, 2021</a>
							</blockquote> 
							<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
						</div>
						<aside class="notes">
							- model-free (e.g. constant, mean, ...)
							- (logistic) regression, random forest, ...
						</aside>
					</section>
					<section>
						<h2>Debugging your<br/>Model</h2>
						<div class="r-stretch r-stack">
							<figure>
								<img src="https://preview.redd.it/g4q983jk7lq21.png?width=1029&format=png&auto=webp&s=4d5d6498b6f48defbe4606576f99b2cd772ba863"
								     alt="test error as function of complexity in traditional vs modern view of overfitting"
									 width="800px" height="200px" style="object-fit: cover; object-position: 0 0;" />
								<figcaption>
									Complexity curves for two models of overfitting 
									(<a href="#belkin19ddescent">Belkin et al., 2019</a>)
								</figcaption>
							</figure>
							<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 960 700" class="fragment">
								<circle cx="150" cy="250" r="50" stroke="red" fill="none" />
								<circle cx="400" cy="250" r="50" stroke="red" fill="none" />
							</svg>
						</div>
						<aside class="notes">
							- start simple!
						</aside>
					</section>
					<section>
						<h2>Debugging your<br/>Model</h2>
						<div class="r-stretch r-stack">
							<pre><code class="python" data-trim>
								import numpy as np


								def my_custom_dl_implementation():
									...
							</code></pre>
							<img src="https://media.giphy.com/media/vyTnNTrs3wqQ0UIvwE/giphy.gif" 
							     alt="no! god please! no! no!" class="fragment" />
						</div>
						<aside class="notes">
							- keep things simple!
						</aside>
					</section>
					<section>
						<h2>Debugging your<br/>Model</h2>
						<div class="r-stretch r-stack">
							<pre><code class="python" data-trim>
								for epoch in range(max_epochs):
									for x, y in train_loader:
										pred = model(x)
										err = loss_func(pred, y)

										opt.zero_grad()
										err.backward()
										opt.step()
									
									with torch.no_grad():
										for x, y in valid_loader:
											pred = model(x)
											val_err = loss_func(pred, y)
							</code></pre>
							<img src="https://media.giphy.com/media/fIkT0LdGUc4GushZ2Q/giphy.gif" 
							     alt="no. no!" class="fragment" />
						</div>
						<aside class="notes">
							- keep code simple (and readable)
						</aside>
					</section>
					<section>
						<h2>Debugging your<br/>Model</h2>
						<div class="r-stretch r-stack">
							<pre><code class="python" data-trim data-line-numbers="1-12|14-24|27-29|26">
								def update(model, loader, loss_func, opt):
									model.train()

									for x, y in loader:
										pred = model(x)
										err = loss_func(pred, y)

										opt.zero_grad()
										err.backward()
										opt.step()
									
									return err.item()
								
								@torch.no_grad()
								def evaluate(model, loader, loss_func):
									model.eval()

									errs = []
									for x, y in loader:
										pred = model(x)
										val_err = loss_func(pred, y)
										errs.append(val_err.item())
									
									return errs
								
								val_errs = evaluate(model, valid_loader, loss_func)
								for epoch in range(1, max_epochs + 1):
									err = update(model, train_loader, loss_func, opt)
									val_errs = evaluate(model, valid_loader, loss_func)
							</code></pre>
							<img src="https://media.giphy.com/media/ei0E9JEqlOT54BI4fk/giphy.gif" 
							     alt="ok" class="fragment"/>
						</div>
						<aside class="notes">
							- keep code simple (and readable)
						</aside>
					</section>
					<section>
						<h2>Debugging your<br/>Model</h2>
						<div class="r-stretch r-stack">
							<iframe src="https://www.pytorchlightning.ai/" width="100%" height="90%">
								<pre><code class="python" data-trim>
									import pytorch_lightning as pl
								</code></pre>
							</iframe>
						</div>
						<aside class="notes">
							- keep code simple (and readable)
						</aside>
					</section>
					<section>
						<h2>Debugging your<br/>Model</h2>
						<div class="r-stretch r-vstack">
							<pre><code class="python" data-trim>
								from torch.utils.data import Subset

								if debug:
									train_data = Subset(data, [0, 1])
									valid_data = Subset(data, range(2, len(data)))
							</code></pre>
							<img src="resources/debug_overfit.png" class="fragment"
							     alt="loss curves for overfitting model" />
						</div>
						<aside class="notes">
							- overfit on small batch of data<br/>
							- plot training/validation loss/metrics<br/>
							- plot different parts of loss individually<br/>
							- visualise activations and/or gradients
						</aside>
					</section>
					<section>
						<h2>choosing your<br/>Model</h2>
						<div class="r-stretch r-stack">
							<figure>
								<img src="https://sgfin.github.io/assets/thesis_images/relational.png" width="500px" height="400px" style="object-fit: cover; object-position: 0 100%;"
								     alt="examples of inductive biases in different architectures" />
								<figcaption>
									Image taken from a  <a href="https://sgfin.github.io/2020/06/22/Induction-Intro/">blog post</a> by Sam Finlayson
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							- inductive biases (what should work)<br/>
							- invariances / equivariances
						</aside>
					</section>
					<section>
						<h2>Choosing your<br/>Model</h2>
						<div class="r-stretch r-stack">
							<figure>
								<img src="https://miro.medium.com/max/1270/1*LpDpZojgoKTPBBt8wdC4nQ.png"
								     alt="attention system in the transformer architecture" width="600px" />
								<figcaption>
									Transformer architecture 
									(<a href="#vaswani17transformer">Vaswani et al., 2017</a>)
								</figcaption>
							</figure>
							<figure class="fragment" style="transform: rotate(-20deg);">
								<img src="https://jalammar.github.io/images/bert-input-output.png"
									 alt="visualisation of the BERT model" width="600px" />
								<figcaption>
									BERT language model
									(<a href="#devlin19bert">Devlin et al., 2019</a>;
									 image from a <a href="https://jalammar.github.io/illustrated-bert/">blog post</a> by Jay Alammar).
								</figcaption>
							</figure>
							<figure class="fragment" style="transform: rotate(30deg);">
								<img src="https://miro.medium.com/max/1400/1*AoE_mecs9_prJ2mIc9TDqQ.png"
									 alt="visualisation of the vision transformer model" width="600px" />
								<figcaption>
									Vision transformer architecture
									(<a href="#dosovitskiy21vit">Dosovitskiy et al., 2021</a>)
								</figcaption>
							</figure>
							<figure class="fragment" style="transform: rotate(-20deg);">
								<img src="https://miro.medium.com/max/770/1*-LddwW7LDDTLrl3CrKDXmQ.jpeg"
									 alt="visualisation of the MLP mixer model" width="600px" />
								<figcaption>
									MLP mixer architecture
									(<a href="#tolstikhin21mlpmixer">Tolstikhin et al., 2021</a>)
								</figcaption>
							</figure>
						</div>
						<p class="citation">
							Hopfield networks is all you need (<a href="#ramsauer21hopfield">Ramsauer et al., 2021</a>)
						</p>
						<aside class="notes">
							- similar to hopfield networks (associative memory)<br/>
							- transfer learning and pre-training<br/>
							- Hyperparameter search (grid vs random)
						</aside>
					</section>
					<section>
						<h2>choosing your<br/>Model</h2>
						<div class="r-stretch r-stack">
							<figure>
								<img src="https://production-media.paperswithcode.com/methods/resnet-e1548261477164.png"
								     alt="skip connections in neural networks" width="600" />
								<figcaption>
									(<a href="#he16resnet">He et al., 2016</a>)
								</figcaption>
							</figure>
						</div>
						<p class="citation">
							(<a href="#vandersmagt98solving">van der Smagt & Hirzinger, 1998</a>;
							 <a href="#srivasta15highway">Srivasta et al., 2015</a>;
							 <a href="#he16resnet">He et al., 2016</a>)
						</p>
						<aside class="notes">
							- skip connections
						</aside>
					</section>
					<!--section>
						<h2>Initialising your<br/>Model</h2>
						<div class="r-stretch r-vstack">
							<p>
								$$\vec{s} = \mat{W} \vec{x} + \vec{b}$$

								<span class="fragment">
									$$S \sim \mathcal{N}(\mu_s, \sigma_s^2)$$
								</span>
							</p>
							<p class="fragment">
								$$\begin{aligned}
									\mu_s &= N \mu_w \mu_x + \mu_b \\
									\sigma_s^2 &= N (\sigma_w^2 \sigma_x^2 + \mu_w^2 \sigma_x^2 + \mu_x^2 \sigma_w^2) + \sigma_b^2
								\end{aligned}$$
							</p>
							<p class="citation">
								See section 4.6 in (<a href="#lecun98efficient">LeCun et al., 1998</a>)
							</p>
						</div>
						<aside class="notes">
							- random weights allow explicit computations of statistics
						</aside>
					</section>
					<section>
						<h2>Initialising your<br/>Model</h2>
						<div class="r-stretch r-vstack">
							<p>
								$$\vec{s} = \mat{W} \vec{x} + \vec{b}$$

								<span>
									$$S \sim \mathcal{N}(\mu_s, \sigma_s^2)$$
								</span>
							</p>
							<p>
								$$\begin{aligned}
									\mu_s &= 0 \\
									\sigma_s^2 &= N \sigma_w^2 (\sigma_x^2 + \mu_x^2)
								\end{aligned}$$
							</p>
							<p class="citation">
								See section 4.6 in (<a href="#lecun98efficient">LeCun et al., 1998</a>)
							</p>
						</div>
						<aside class="notes">
							- use zero-mean weights and set biases to zero
						</aside>
					</section>
					<section>
						<h2>Initialising your<br/>Model</h2>
						<div class="r-stretch r-vstack">
							<p>
								$$\begin{aligned}
									\mu_s &= 0 \\
									\sigma_s^2 &= N \sigma_w^2 (\sigma_x^2 + \mu_x^2)
								\end{aligned}$$
							</p>
							<p class="fragment">
								$$\begin{align}
									\vec{x} &= \vec{s}^- &\Rightarrow 
									\mu_x^2 + \sigma_s^2 & = \sigma_{s^-}^2 \\
									\vec{x} &= \operatorname{ReLU}(\vec{s}^-) & \Rightarrow
									\mu_x^2 + \sigma_x^2 &= \frac{1}{2} \sigma_{s^-}^2
								\end{align}$$
							</p>
							<p class="citation">
								(<a href="#lecun98efficient">LeCun et al., 1998</a>;
								 <a href="#he15delving">He et al., 2015</a>)
							</p>
						</div>
						<aside class="notes">
							- distribution of x depends on activation function<br/>
							- effectively induces steady propagation<br/>
							- does not work quite the same way as data normalisation
						</aside>
					</section>
					<section>
						<h2>Initialising your<br/>Model</h2>
						<div class="r-stretch r-vstack">
							<p>
								$$\vec{a} = \phi(\vec{s})$$

								<span class="fragment">
									$$A \sim ?$$
								</span>
							</p>
							<p class="fragment">
								$$\begin{aligned}
									\vec{a} & = \operatorname{ReLU}(\vec{s}) &\Rightarrow
									&\begin{cases}
										\mu_a &= \frac{1}{\sqrt{2 \pi}} \\ \sigma_a^2 &= \frac{1}{2} \sigma_s^2 - \frac{1}{2 \pi}
									\end{cases}
								\end{aligned}$$
							</p>
						</div>
						<aside class="notes">
							- proper initialisation for same reason as data normalisation<br/>
							- descrepancy between forward and backward pass<br/>
						</aside>
					</section>
					<section>
						<h2>Initialising your<br/>Model</h2>
						<div class="r-stretch r-vstack">
							<p>
								$$\begin{aligned}
									\vec{\delta}^- &= (\vec{\delta} \mat{W}) \odot \phi'(\vec{s}^-) \\
									&\approx \vec{\delta} \mat{W}
								\end{aligned}$$
							</p>
							<p class="fragment">
								$$\begin{aligned}
									\mu_\delta &\approx M \mu_\delta \mu_w \\
									\sigma_\delta^2 &\approx M \sigma_\delta^2 \sigma_w^2
								\end{aligned}$$
							</p>
						</div>
						<p class="citation">
							(<a href="#glorot10understanding">Glorot & Bengio, 2010</a>)
						</p>
						<aside class="notes">
							- descrepancy between forward and backward pass
						</aside>
					</section-->
					<section>
						<h2>Initialising your<br/>Model</h2>
						<div class="r-stretch r-vstack">
							<ul>
								<li>
									$\sigma_w^2 = \frac{1}{N}$ 
									<span style="float: right">(<a href="#lecun98efficient">Lecun et al., 1998</a>)</span>
								</li>
								<li>
									$\sigma_w^2 = \frac{2}{N}$ or $\sigma_w^2 = \frac{2}{M}$ 
									<span style="float: right">(<a href="#he15delving">He et al., 2015</a>)</span>
								</li>
								<li>
									$\sigma_w^2 = \frac{2}{N + M}$ 
									<span style="float: right">(<a href="#glorot10understanding">Glorot & Bengio, 2010</a>)</span>
								</li>
								<li>
									$\sigma_w^2 = \frac{1}{\sqrt{N M}}$ 
									<span style="float: right">(<a href="#defazio21beyond">Defazio & Bottou, 2021</a>)</span>
								</li>
							</ul>
						</div>
						<aside class="notes">
							- descrepancy between forward and backward pass
						</aside>
					</section>
					<section>
						<h2>Initial biases in your<br/>Model</h2>
						<div class="r-stretch r-vstack justify-around">
							<div>
								$$\vec{x} = \vec{0} \Rightarrow \hat{\vec{y}} = \sigma(\vec{b})$$
	
								<ul>
									<li>Balanced data: $\vec{b} = \vec{0}$</li>
									<li>Unbalanced data: $b_k \propto \ln\Big(\frac{p(y_k)}{1 - p(y_k)}\Big)$</li>
								</ul>
							</div>
							<p class="fragment">
								"open" <em>LSTM</em> gate: $\vec{b} \gg 0$ <br/>
								"closed" <em>LSTM</em> gate: $\vec{b} \ll 0$
							</p>
						</div>
						<aside class="notes">
							- initial bias values can model priors
						</aside>
					</section>
					<section>
						<h2>Normalising your<br/>Model</h2>
						<div class="r-stretch r-vstack">
							<p>
								Batch-normalisation:

								$$\begin{aligned}
									\hat{\vec{x}} &= \frac{\vec{x} - \vec{\mu}_\mathcal{B}}{\vec{\sigma}_\mathcal{B}}\\
									\vec{y} &= \vec{\gamma} \odot \vec{x} + \vec{\beta}
								\end{aligned}$$
							</p>
							<ul>
								<li>before activation function (<a href="#ioffe15batchnorm">Ioffe et al., 2015</a>)</li>
								<li>after activation function (<a href="#mishkin16lsuv">Mishkin & Matas, 2016</a>)</li>
							</ul>
						</div>
						<p class="citation">
							(<a href="#schraudolph98centering">Schraudolph, 1998</a>;
							 <a href="#ioffe15batchnorm">Ioffe et al., 2015</a>)
						</p>
						<aside class="notes">
							- famous due to batch-normalisation<br/>
							- allows larger learning rates<br/>
							- scale invariant: makes initialisation less important
						</aside>
					</section>
					<section>
						<h2>Normalising your<br/>Model</h2>
						<div class="r-stretch r-stack">
							<figure>
								<img src="https://iclr-blog-track.github.io/public/images/2022-03-25-unnormalized-resnets/normalisation_dimensions.svg" 
								     width="900" style="object-fit: cover; object-position: 0 0;"
								     alt="visualisation of multiple normalisation methods" />
								<figcaption>
									different axes of normalisation 
									(<a href="#hoedt22normalisation">Hoedt et al., 2022</a>; <a href="#wu18group">Wu & He, 2018</a>)
								</figcaption>
							</figure>
							<div class="fragment" style="background-color: white;">
								<p>
									Alternatives

									<ul>
										<li>
											Layer normalisation 
											<span style="float: right">(<a href="#ba16layernorm">Ba et al., 2016</a>)</span>
										</li>
										<li>
											Weight normalisation 
											<span style="float: right">(<a href="#salimans16weightnorm">Salimans & Kingma, 2016</a>)</span>
										</li>
										<li>
											Self-normalisation 
											<span style="float: right">(<a href="#klambauer17selfnorm">Klambauer et al., 2017</a>)</span>
										</li>
										<li>...</li>
									</ul>
								</p>
							</div>
						</div>
						<aside class="notes">
							- layer norm: for RNNs (not so good for CNN)<br/>
							- weight norm: only scales (no mean-shift)<br/>
							- self norm: induced normalisation
						</aside>
					</section>
					<section>
						<h2>TL;DR<br/>Model</h2>
						<div class="r-stretch r-stack">
							<ul>
								<li>start as <strong>simple</strong> as possible</li>
								<li>use <strong>prior</strong> knowledge (or don't)</li>
								<li>mind the <strong>signal flow</strong>!</li>
							</ul>
						</div>
						<aside class="notes">
							TODO
						</aside>
					</section>
				</section>

				<section>
					<section>
						<h2>Outline</h2>
						<div class="r-stretch r-stack">
							<ol>
								<li class="irrelevant">Data</li>
								<li class="irrelevant">Model</li>
								<li>Learning</li>
							</ol>
						</div>
					</section>
					<section>
						<h2>Optimisation algorithms<br/>Learning</h2>
						<div class="r-stretch r-hstack">
							<figure>
								<img src="https://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif"
								     alt="animation of various optimisation algorithms on the Beale function (contours)" width="400" />
								<figcaption>
									A collection of optimisation algorithms on the Beale function.
								</figcaption>
							</figure>
							<figure>
								<img src="https://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif"
								     alt="animation of various optimisation algorithms in a saddle point (3D)" width="400" />
								<figcaption>
									Same collection of optimisation algorithms in a saddle point.
								</figcaption>
							</figure>
						</div>
						<p class="citation">
							Images from the OG 
							<a href="https://ruder.io/optimizing-gradient-descent/">blog post</a> by Sebastian Ruder
						</p>
					</section>
					<section>
						<h2>Saddle points during<br/>Learning</h2>
						<div class="r-stretch r-stack">
							<figure>
								<img src="https://discuss.pytorch.org/uploads/default/af84210c66d3a2e87a38f419f248fd79999d8c1a"
								     alt="LSTM training curve with plateau" />
								<figcaption>
									typical LSTM training curve (image from the <a href="https://discuss.pytorch.org/t/werid-training-loss-curve/8799">pytorch forums</a>)
								</figcaption>
							</figure>
						</div>
					</section>
					<section>
						<h2>Rate of<br/>Learning</h2>
						<div class="r-stretch r-hstack">
							<figure>
								<img src="resources/lr_importance.png"
								     alt="learning curves for different learning rates" />
								<figcaption>
									Learning curves for different learning rates
								</figcaption>
							</figure>
							<blockquote cite="Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.">
								If you have time to tune only one hyperparameter, tune the learning rate.
							</blockquote>
						</div>
						<p class="citation">
							Deep Learning Book (<a href="#goodfellow16dl">Goodfellow et al., 2016 </a>)
						</p>
					</section>
					<section>
						<h2>Rate of<br/>Learning</h2>
						<div class="r-stretch r-hstack">
							<div class="r-vstack">
								<figure>
									<img src="https://d2l.ai/_images/output_lr-scheduler_1dfeb6_41_0.svg"
										 alt="sqrt learning rate schedule" width="250px" />
									<figcaption>
										$\eta(t) = \frac{1}{\sqrt{t + 1}} \eta$
									</figcaption>
								</figure>
								<figure> 
									<img src="https://d2l.ai/_images/output_lr-scheduler_1dfeb6_68_0.svg"
										 alt="exponential learning rate schedule" width="250px" />
									<figcaption>
										$\eta(t) = \alpha^t \eta$
									</figcaption>
								</figure>
							</div>
							<div class="r-vstack">
								<figure>
									<img src="https://d2l.ai/_images/output_lr-scheduler_1dfeb6_80_0.svg"
										 alt="stepping learning rate schedule" width="250px" />
									<figcaption>
										$\eta(t) = \alpha^{\lfloor t / N \rfloor} \eta$
									</figcaption>
								</figure>
								<figure>
									<img src="https://d2l.ai/_images/output_lr-scheduler_1dfeb6_104_0.svg"
										 alt="cosine learning rate schedule" width="250px" />
									<figcaption>
										$\eta(t) = \eta_T + \frac{\eta_0 - \eta_T}{2} \big(1 + \cos(\pi \frac{t}{T})\big)$
									</figcaption>
								</figure>
							</div>
							<figure class="fragment">
								<img src="https://d2l.ai/_images/output_lr-scheduler_1dfeb6_125_0.svg"
									 alt="cosine learning rate schedule" width="250px" />
								<figcaption>
									warmup
								</figcaption>
							</figure>
						</div>
						<p class="citation">
							(<a href="#loshchilov17sgdr">Loshchilov & Hutter, 2017</a>; &copy; 
							 images from (<a href="#zhang20d2l">Zhang et al., 2021</a>))
						</p>
					</section>
					<section>
						<h2>Rate of<br/>Learning</h2>
						<div class="r-stack r-stretch">
							<pre style="width: 600px"><code class="python" data-trim>
								ce = nn.CrossEntropyLoss(reduction="mean")
							</code></pre>
							<figure class="fragment">
								<img src="resources/updates_vs_epochs.png" width="600px"
								     alt="learning curves as functions of epoch vs update" />
								<figcaption>
									Learning curves for different batch sizes (and number of epochs)
								</figcaption>
							</figure>
						</div>
						<aside class="notes">
							when optimising mean of errors:
							number of updates more important than number of epochs!
						</aside>
					</section>
					<section>
						<h2>Rate of<br/>Learning</h2>
						<div class="r-stack r-stretch">
							<figure>
								<img src="resources/updates_vs_epochs_summed.png" width="600px"
								     alt="learning curves as functions of epoch vs update" />
								<figcaption>
									Learning curves for different batch sizes (and number of epochs)
								</figcaption>
							</figure>
							<pre class="fragment" style="width: 600px"><code class="python" data-trim>
								ce = nn.CrossEntropyLoss(reduction="sum")
							</code></pre>
						</div>
						<aside class="notes">
							when optimising sum of errors:
							number of epochs more important than number of updates!
						</aside>
					</section>
					<section>
						<h2>Rate of<br/>Learning</h2>
						<div class="r-stretch r-hstack">
							<figure>
								<img src="https://ruder.io/content/images/2017/05/mtl_images-001-2.png"
								     alt="diagram depicting multi-task learning" />
								<figcaption>
									Multi-task learning (image from a <a href="https://ruder.io/multi-task/index.html#twomtlmethodsfordeeplearning">blog post</a> from Sebastian Ruder)
								</figcaption>
							</figure>
							<figure>
								<img src="https://he-s3.s3.amazonaws.com/media/uploads/67d854f.png"
								     alt="example of semantic segmentation for self-driving cars" />
								<figcaption>
									Segmentation (image from <a href="https://pydata.org/delhi2017/schedule/presentation/23/">pydata 2017 session</a>)
								</figcaption>
							</figure>
						</div>
						<p class="citation">
							(<a href="#caruana98multitask">Caruana, 1998</a>)
						</p>
						<aside class="notes">
							- more tasks = more learning signal<br/>
							- segmentation = every pixel is a task
						</aside>
					</section>
					<section>
						<h2>Regularised<br/>Learning</h2>
						<div class="r-stretch r-stack">
							<figure>
								<img src="https://preview.redd.it/g4q983jk7lq21.png?width=1029&format=png&auto=webp&s=4d5d6498b6f48defbe4606576f99b2cd772ba863"
								     alt="test error as function of complexity in traditional vs modern view of overfitting"
									 width="800px" height="200px" style="object-fit: cover; object-position: 0 0;" />
								<figcaption>
									Complexity curves for two models of overfitting 
									(<a href="#belkin19ddescent">Belkin et al., 2019</a>)
								</figcaption>
							</figure>
							<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 960 700" class="fragment">
								<circle cx="230" cy="300" r="50" stroke="red" fill="none" />
								<circle cx="810" cy="300" r="50" stroke="red" fill="none" />
							</svg>
						</div>
						<p class="citation">
							(<a href="#prechelt98earlystop">Prechelt, 1998</a>)
						</p>
						<aside class="notes">
							- early stopping mainly motivated by old model<br/>
							- trade-off between training time and performance<br/>
							- chapter 2 in tricks of the trade
						</aside>
					</section>
					<section>
						<h2>Regularised<br/>Learning</h2>
						<div class="r-stretch r-stack">
							<figure>
								<img src="https://files.ai-pool.com/a/8d6f0a2aab0ca87048d9532797174f1d.png"
								     alt="standard neural network vs network with dropout" width="650" />
								<figcaption>
									Dropout regularisation
									(<a href="#srivasta14dropout">Srivasta et al., 2014</a>)
								</figcaption>
							</figure>
						</div>
						<p class="citation">
							(<a href="#gal16dropout">Gal & Ghahramani, 2016</a>; 
							 <a href="#li19understanding">Li et al., 2019</a>)
						</p>
						<aside class="notes">
							- dropout create ensemble of partial networks<br/>
							- allows for uncertainty estimation<br/>
							- does not work well with Batch-normalisation
						</aside>
					</section>
					<section>
						<h2>Regularised<br/>Learning</h2>
						<div class="r-stretch r-hstack">
							<figure>
								<img src="https://miro.medium.com/max/1400/1*BDXCF-C9azGzr-OFzBFlEw.png"
								     alt="multi-class labels without and with label smoothing" />
								<figcaption>
									Label smoothing
									(image from a <a href="https://towardsdatascience.com/label-smoothing-make-your-model-less-over-confident-b12ea6f81a9a">blog post</a> by Parthvi Shah)
								</figcaption>
							</figure>
							<figure>
								<img src="https://production-media.paperswithcode.com/methods/image3_1_oTiwmLN.png"
								     alt="2D projections of penultimate activations without and with label smoothing" />
								<figcaption>
									Label smoothing
									(<a href="#mueller19labels">Müller et al., 2019</a>)
								</figcaption>
							</figure>
						</div>
						<p class="citation">
							(<a href="#szegedy16rethinking">Szegedy et al., 2016</a>)
						</p>
						<aside class="notes">
							- label smoothing attempts to prevent over-confident predictions
						</aside>
					</section>
					<section>
						<h2>Regularised<br/>Learning</h2>
						<div class="r-stretch r-hstack">
							<figure>
								<img src="https://upload.wikimedia.org/wikipedia/commons/f/f8/L1_and_L2_balls.svg"
								     alt="unit balls in L1, L2 norms and how they affect parameters" />
								<figcaption>
									unit balls in different norms and how they affect parameters <br/>
									(image taken from <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">wikipedia)</a>
								</figcaption>
							</figure>
							<p>
								$$\min_\theta L(g(\vec{x} \mathbin{;} \theta), y) + \lambda \|\theta\|$$
							</p>
						</div>
						<p class="citation">
							(<a href="#hanson89comparing">Hanson & Pratt, 1989</a>; 
							 <a href="#roegnvaldsson98simple">Rögnvaldsson, 1998</a>; 
							 <a href="#loshchilov19decoupled">Loshchchilov & Hutter, 2019</a>)
						</p>
						<aside class="notes">
							- weight decay aims to keep weights small<br/>
							- equivalent to L2 regularisation (with SGD)<br/>
							- tricks of the trade: use early stopping to set parameter<br/>
							- equivalence breaks with modern optimisers
						</aside>
					</section>
					<section>
						<h2>TL;DR<br/>Learning</h2>
						<div class="r-stretch r-stack">
							<ul>
								<li>adaptive optimisers are <strong>easier</strong> (SGD is better)</li>
								<li>maximise the <strong>rate</strong> of learning</li>
								<li>use <strong>regularisation</strong> (after overfitting)</li>
							</ul>
						</div>
						<aside class="notes">
							TODO
						</aside>
					</section>
				</section>

				<section>
					<h2>TL;DR</h2>
					<div class="r-vstack r-stretch">
						<ol>
							<li>Understand your data</li>
							<li>Keep your model simple</li>
							<li>Learning benefits from tuning</li>
						</ol>
					</div>
				</section>

				<section id="references" data-state="no-footer">
					<h3>References</h3>
					<div class="references r-stretch">
						<p id="ba16layernorm">
							Ba et al. (2016). Layer Normalization [Whitepaper]. 
							(<a href="http://arxiv.org/abs/1607.06450">link</a>,
							 <a href="http://arxiv.org/pdf/1607.06450.pdf">pdf</a>)
						</p>
						<p id="belkin19ddescent">
							Belkin et al. (2019). Reconciling modern machine-learning practice and the classical bias–variance trade-off. 
							Proceedings of the National Academy of Sciences, 116(32), 15849–15854. 
							(<a href="https://doi.org/10.1073/pnas.1903070116">link</a>,
							 <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">pdf</a>)
						</p>		
						<p id="bottou94mnist">
							Bottou et al. (1994). Comparison of classifier methods: A case study in handwritten digit recognition. 
							Proceedings of the 12th IAPR International Conference on Pattern Recognition, 2, 77–82. 
							(<a href="https://doi.org/10.1109/ICPR.1994.576879">link</a>,
							 <a href="https://leon.bottou.org/publications/pdf/icpr-1994.pdf">pdf</a>)
						</p>
						<p id="caruana98multitask">
							Caruana (1998). A Dozen Tricks with Multitask Learning. 
							In G. B. Orr & K.-R. Müller (Eds.), Neural Networks: Tricks of the Trade (1st ed., pp. 165–191). Springer.
							(<a href="https://doi.org/10.1007/3-540-49430-8_9">link</a>)
						</p>
						<p id="defazio21beyond">
							Defazio & Bottou (2021). Beyond Folklore: A Scaling Calculus for the Design and Initialization of ReLU Networks [Preprint]. 
							(<a href="http://arxiv.org/abs/1906.04267">link</a>,
							 <a href="http://arxiv.org/pdf/1906.04267.pdf">pdf</a>
						</p>
						<p id="devlin19bert">
							Devlin et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 
							Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 1, 4171–4186. 
							(<a href="https://doi.org/10.18653/v1/N19-1423">link</a>,
							 <a href="https://aclanthology.org/N19-1423.pdf">pdf</a>)
						</p>
						<p id="dosovitskiy21vit">
							Dosovitskiy et al. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. 
							International Conference on Learning Representations, 9. 
							(<a href="https://openreview.net/forum?id=YicbFdNTTy">link</a>,
							 <a href="https://openreview.net/images/pdf_icon_blue.svg">pdf</a>)
						</p>
						<p id="glorot10understanding">
							Glorot & Bengio (2010). Understanding the difficulty of training deep feedforward neural networks. 
							Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 9, 249–256. 
							(<a href="http://proceedings.mlr.press/v9/glorot10a.html">link</a>,
							 <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">pdf</a>)
						</p>
						<p id="goodfellow16dl">
							Goodfellow et al. (2016). Deep Learning. MIT Press.
							(<a href="https://www.deeplearningbook.org">link</a>)
						</p>
						<p id="grother95nist">
							Grother & Hanaoka (1995). Handprinted Forms and Characters Database (Special Database No. 19). 
							National Institute of Standards and Technology. 
							(<a href="https://doi.org/10.18434/T4H01C">link</a>,
							<a href="https://s3.amazonaws.com/nist-srd/SD19/1stEditionUserGuide.pdf">pdf</a>)
						</p>
						<p id="hanson89comparing">
							Hanson & Pratt (1989). Comparing Biases for Minimal Network Construction with Back-Propagation. 
							Advances in Neural Information Processing Systems, 1, 177–185. 
							(<a href="https://papers.nips.cc/paper/1988/hash/1c9ac0159c94d8d0cbedc973445af2da-Abstract.html">link</a>,
							 <a href="https://papers.nips.cc/paper/1988/file/1c9ac0159c94d8d0cbedc973445af2da-Paper.pdf">pdf</a>)
						</p>
						<p id="he15delving">
							He et al. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. 
							Proceedings of the IEEE International Conference on Computer Vision, 1026–1034. 
							(<a href="https://doi.org/10.1109/ICCV.2015.123">link</a>,
							 <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">pdf</a>)
						</p>
						<p id="he16resnet">
							He et al. (2016). Identity Mappings in Deep Residual Networks. In B. Leibe, J. Matas, N. Sebe, & M. Welling (Eds.), 
							Computer Vision – ECCV 2016 (pp. 630–645). Springer International Publishing. 
							(<a href="https://doi.org/10.1007/978-3-319-46493-0_38">link</a>,
							 <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">pdf</a>)
						</p>
						<p id="hoedt22normalisation">
							Hoedt et al. (2022). Normalization is dead, long live normalization!.
							ICLR Blog Track.
							(<a href="https://iclr-blog-track.github.io/2022/03/25/unnormalized-resnets/">link</a>) 
						</p>
						<p id="ioffe15batchnorm">
							Ioffe & Szegedy (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. 
							Proceedings of the 32nd International Conference on Machine Learning, 37, 448–456. 
							(<a href="http://proceedings.mlr.press/v37/ioffe15.html">link</a>,
							 <a href="http://proceedings.mlr.press/v37/ioffe15.pdf">pdf</a>)
						</p>
						<p id="klambauer17selfnorm">
							Klambauer et al. (2017). Self-Normalizing Neural Networks. 
							Advances in Neural Information Processing Systems, 30, 971–980. 
							(<a href="https://papers.nips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html">link</a>,
							 <a href="https://papers.nips.cc/paper/2017/file/5d44ee6f2c3f71b73125876103c8f6c4-Paper.pdf">pdf</a>)
						</p>
						<p id="lecun98efficient">
							LeCun et al. (1998). Efficient BackProp. 
							In G. B. Orr & K.-R. Müller (Eds.), Neural Networks: Tricks of the Trade (1st ed., pp. 9–50). Springer. 
							(<a href="https://doi.org/10.1007/3-540-49430-8_2">link</a>,
							 <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">pdf</a>)
						</p>
						<p id="li19understanding">
							Li et al. (2019). Understanding the Disharmony Between Dropout and Batch Normalization by Variance Shift. 
							2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2677–2685. 
							(<a href="https://doi.org/10.1109/CVPR.2019.00279">link</a>,
							 <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.pdf">pdf</a>)
						</p>
						<p id="loshchilov17sgdr">
							Loshchilov & Hutter (2017). SGDR: Stochastic Gradient Descent with Warm Restarts. 
							International Conference on Learning Representations 5.
							(<a href="https://openreview.net/forum?id=Skq89Scxx">link</a>,
							 <a href="https://openreview.net/pdf?id=Skq89Scxx">pdf</a>)
						</p>
						<p id="loshchilov19decoupled">
							Loshchilov, & Hutter (2018). Decoupled Weight Decay Regularization. 
							International Conference on Learning Representations 6. 
							(<a href="https://openreview.net/forum?id=Bkg6RiCqY7">link</a>,
							 <a href="https://openreview.net/pdf?id=Bkg6RiCqY7">pdf</a>)
						</p>
						<p id="mishkin16lsuv">
							Mishkin et al. (2016). All you need is a good init. 
							International Conference on Learning Representations 4. 
							(<a href="http://arxiv.org/abs/1511.06422">link</a>,
							 <a href="http://arxiv.org/pdf/1511.06422.pdf">pdf</a>)
						</p>
						<p id="mueller19labels">
							Müller et al. (2019). When does label smoothing help? 
							Advances in Neural Information Processing Systems, 32, 4694–4703.
							(<a href="https://papers.nips.cc/paper/2019/hash/f1748d6b0fd9d439f71450117eba2725-Abstract.html">link</a>,
							 <a href="https://papers.nips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf">pdf</a>)
						</p>
						<p id="prechelt98earlystop">
							Prechelt (1998). Early Stopping—But When? 
							In G. B. Orr & K.-R. Müller (Eds.), Neural Networks: Tricks of the Trade (1st ed., pp. 55–69). Springer. 
							(<a href="https://doi.org/10.1007/3-540-49430-8_3">link</a>,
							 <a href="http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf">pdf</a>)
						</p>
						<p id="orr98tricks">
							Orr & Müller (Eds.). (1998). Neural Networks: Tricks of the Trade (1st ed., Vol. 1524). Springer. 
							(<a href="https://link.springer.com/book/10.1007/3-540-49430-8">link</a>)
						</p>
						<p id="ramsauer21hopfield">
							Ramsauer et al. (2021). Hopfield Networks is All You Need. 
							International Conference on Learning Representations 9. 
							(<a href="https://openreview.net/forum?id=tL89RnzIiCd">link</a>,
							 <a href="https://openreview.net/pdf?id=tL89RnzIiCd">pdf</a>)
						</p>
						<p id="roegnvaldsson98simple">
							Rognvaldsson (1998). A Simple Trick for Estimating the Weight Decay Parameter. 
							In G. B. Orr & K.-R. Müller (Eds.), Neural Networks: Tricks of the Trade (1st ed., pp. 71–92). Springer. 
							(<a href="https://doi.org/10.1007/3-540-49430-8_4">link</a>)
						</p>
						<p id="salimans16weightnorm">
							Salimans & Kingma (2016). Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks. 
							Advances in Neural Information Processing Systems, 29, 901–909. 
							(<a href="https://proceedings.neurips.cc/paper/2016/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html">link</a>,
							 <a href="https://proceedings.neurips.cc/paper/2016/file/ed265bc903a5a097f61d3ec064d96d2e-Paper.pdf">pdf</a>)
						</p>
						<p id="schraudolph98centering">
							Schraudolph (1998). Centering Neural Network Gradient Factors. 
							In G. B. Orr & K.-R. Müller (Eds.), Neural Networks: Tricks of the Trade (1st ed., pp. 207–226). Springer. 
							(<a href="https://doi.org/10.1007/3-540-49430-8_11">link</a>,
							 <a href="https://n.schraudolph.org/pubs/Schraudolph98.pdf">pdf</a>)
						</p>
						<p id="srivasta14dropout">
							Srivastava et al. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. 
							Journal of Machine Learning Research, 15(56), 1929–1958.
							(<a href="https://jmlr.org/papers/v15/srivastava14a.html">link</a>,
							 <a href="https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">pdf</a>)
						</p>
						<p id="srivasta15highway">
							Srivastava et al. (2015). Training Very Deep Networks. 
							Advances in Neural Information Processing Systems, 28, 2377–2385. 
							(<a href="https://papers.nips.cc/paper/2015/hash/215a71a12769b056c3c32e7299f1c5ed-Abstract.html">link</a>,
							 <a href="https://papers.nips.cc/paper/2015/file/215a71a12769b056c3c32e7299f1c5ed-Paper.pdf">pdf</a>)
						</p>
						<p id="tolstikhin21mlpmixer">
							Tolstikhin et al. (2021). MLP-Mixer: An all-MLP Architecture for Vision [Preprint]. 
							(<a href="http://arxiv.org/abs/2105.01601">link</a>,
							 <a href="http://arxiv.org/pdf/2105.01601.pdf">pdf</a>)
						</p>
						<p id="vandersmagt98solving">
							van der Smagt & Hirzinger (1998). Solving the Ill-Conditioning in Neural Network Learning. 
							In G. B. Orr & K.-R. Müller (Eds.), Neural Networks: Tricks of the Trade (1st ed., pp. 193–206). Springer. 
							(<a href="https://doi.org/10.1007/3-540-49430-8_10">link</a>)
						</p>
						<p id="vaswani17transformer">
							Vaswani et al. (2017). Attention Is All You Need. 
							Advances in Neural Information Processing Systems, 30, 5998–6008. 
							(<a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">link</a>,
							 <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">pdf</a>)
						</p>
						<p id="wadia21whitening">
							Wadia et al. (2021). Whitening and Second Order Optimization Both Make Information in the Dataset Unusable During Training, and Can Reduce or Prevent Generalization. 
							Proceedings of the 38th International Conference on Machine Learning, 139, 10617–10629. 
							(<a href="http://proceedings.mlr.press/v139/wadia21a.html">link</a>,
							 <a href="http://proceedings.mlr.press/v139/wadia21a/wadia21a.pdf">pdf</a>)
						</p>
						<p id="wang21pacmap">
							Wang et al. (2021). Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMap, and PaCMAP for Data Visualization. 
							Journal of Machine Learning Research, 22(201), 1–73.
							(<a href="https://jmlr.org/papers/v22/20-1061.html">link</a>,
							 <a href="https://jmlr.org/papers/volume22/20-1061/20-1061.pdf">pdf</a>)
						</p>
						<p id="wu18group">
							Wu & He (2018). Group Normalization. In V. Ferrari, M. Hebert, C. Sminchisescu, & Y. Weiss (Eds.), 
							Computer Vision – ECCV 2018 (pp. 3–19). Springer International Publishing. 
							(<a href="https://doi.org/10.1007/978-3-030-01261-8_1">link</a>,
							 <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Yuxin_Wu_Group_Normalization_ECCV_2018_paper.pdf">pdf</a>)
						</p>
						<p id="yadav19qmnist">
							Yadav & Bottou (2019). Cold Case: The Lost MNIST Digits. 
							Advances in Neural Information Processing Systems, 32, 13443–13452. 
							(<a href="https://proceedings.neurips.cc/paper/2019/hash/51c68dc084cb0b8467eafad1330bce66-Abstract.html">link</a>,
							 <a href="https://proceedings.neurips.cc/paper/2019/file/51c68dc084cb0b8467eafad1330bce66-Paper.pdf">pdf</a>)
						</p>
						<p id="zhang20d2l">
							Zhang et al. (2020). Dive into Deep Learning.
							(<a href="https://d2l.ai/">link</a>,
							 <a href="https://arxiv.org/pdf/2106.11342">pdf</a>)
						</p>
					</div>
				</section>
			</main>
			<footer>
				<img src="https://www.jku.at/fileadmin/marketing/Startseite/JKU-Hauptlogo-en-schwarz-quer.svg" 
					 alt="JKU footer logo" style="height: 20px;" />
				<span></span>
			</footer>
		</div>

		<script src="https://unpkg.com/reveal.js/dist/reveal.js"></script>
		<script src="https://unpkg.com/reveal.js/plugin/notes/notes.js"></script>
		<script src="https://unpkg.com/reveal.js/plugin/math/math.js"></script>
		<script src="https://unpkg.com/reveal.js/plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				slideNumber: true,
				autoAnimateDuration: 0.,
				transition: 'none',

				math: {
					TeX: { Macros: { 
						vec: ["\\boldsymbol{#1}", 1],
						mat: ["\\boldsymbol{#1}", 1],
						E: ["\\mathbb{E}"],
					} }
				},

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMath, RevealHighlight, RevealNotes ]
			});

			Reveal.on( 'ready', event => {
				footer = Reveal.getRevealElement().querySelector( "footer" );
				footer.style.transform = 'scale(' + Reveal.getScale() + ')';

				Reveal.on( 'resize', event => {
					console.log(footer)
					console.log(event.scale)
					footer.style.transform = 'scale('+ event.scale +')';
				});
			})
		</script>
	</body>
</html>
